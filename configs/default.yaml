# Default configuration for adaptive feature importance reweighting

# Random seed for reproducibility
seed: 42

# Data configuration
data:
  dataset_name: "home_credit"
  train_size: 0.7
  val_size: 0.15
  test_size: 0.15
  sample_frac: 1.0  # Use 1.0 for full dataset, <1.0 for quick testing
  min_samples: 10000  # Minimum samples to use if dataset is too small

# Model configuration
model:
  type: "ensemble"  # ensemble, lightgbm, xgboost, catboost

  # LightGBM parameters
  lightgbm:
    boosting_type: "gbdt"
    num_leaves: 31
    max_depth: 6
    learning_rate: 0.05
    n_estimators: 500
    min_child_samples: 20
    subsample: 0.8
    subsample_freq: 1
    colsample_bytree: 0.8
    reg_alpha: 0.1
    reg_lambda: 0.1
    random_state: 42
    n_jobs: -1
    importance_type: "gain"

  # XGBoost parameters
  xgboost:
    max_depth: 6
    learning_rate: 0.05
    n_estimators: 500
    min_child_weight: 1
    subsample: 0.8
    colsample_bytree: 0.8
    reg_alpha: 0.1
    reg_lambda: 0.1
    random_state: 42
    n_jobs: -1
    tree_method: "hist"
    enable_categorical: true

  # CatBoost parameters
  catboost:
    iterations: 500
    depth: 6
    learning_rate: 0.05
    l2_leaf_reg: 0.1
    subsample: 0.8
    random_state: 42
    verbose: 0
    thread_count: -1
    task_type: "CPU"

  # Ensemble weights (must sum to 1.0)
  ensemble_weights:
    lightgbm: 0.4
    xgboost: 0.35
    catboost: 0.25

# Adaptive reweighting configuration
reweighting:
  enabled: true
  strategy: "shap"  # shap, permutation, gradient
  update_frequency: 5  # Update importance every N epochs
  warmup_epochs: 10  # Number of epochs before enabling reweighting
  top_k_features: 20  # Focus on top K most important features
  reweight_alpha: 0.3  # Strength of reweighting (0=none, 1=full)
  temporal_decay: 0.95  # Decay factor for historical importance
  segment_bins: 5  # Number of risk segments for stratified reweighting
  min_importance_threshold: 0.01  # Minimum importance to consider

# Curriculum learning configuration
curriculum:
  enabled: true
  warmup_epochs: 15
  initial_easy_ratio: 0.7  # Start with 70% easy samples
  final_easy_ratio: 0.3  # End with 30% easy samples
  difficulty_metric: "prediction_variance"  # prediction_variance, loss, uncertainty
  schedule: "linear"  # linear, cosine, exponential

# Training configuration
training:
  num_boost_round: 100  # Number of boosting rounds (epochs)
  early_stopping_rounds: 20
  verbose_eval: 10
  use_best_iteration: true

# Optimization configuration
optimization:
  enabled: false  # Set to true to run hyperparameter optimization
  n_trials: 50
  timeout: 3600  # 1 hour
  n_jobs: 1
  sampler: "tpe"  # tpe, random, grid

# Evaluation metrics
metrics:
  primary: "roc_auc"
  track:
    - "roc_auc"
    - "pr_auc"
    - "ks_statistic"
    - "gini"
    - "brier_score"

# Logging configuration
logging:
  level: "INFO"
  log_to_file: true
  log_dir: "logs"
  mlflow:
    enabled: true
    experiment_name: "adaptive_feature_importance"
    tracking_uri: "file:./mlruns"

# Checkpoint configuration
checkpoint:
  save_dir: "models"
  save_best_only: true
  monitor: "roc_auc"
  mode: "max"

# Output configuration
output:
  results_dir: "results"
  save_predictions: true
  save_feature_importance: true
  save_shap_values: false  # Set to true to save SHAP values (large files)
  generate_plots: true
